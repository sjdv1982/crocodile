The very highest level is the task level. Parameters to reflect the user intention. 
For normal users. Can be converted into web server.
Status: Delay for the last.

Conversion between very highest and high level: Straightforward-ish, but low priority.

The high level is the model level. Model parameters with sufficient detail to arrive at unique chains 
(but not in a unique way; e.g. the merge point is not defined)
For power users. 
Status: Delay until later.



The middle level is the command level. The command list reflects the concrete actions to be taken.
For example, where to merge the chains, and which filter to apply in which order.
Status: Write by hand for the time being. Conversion between high and middle level is extremely difficult.

Conversion between middle and low level: high-ish priority. Requires dependency tracking: 
what can be executed right now, but also correct re-use of folders upon change of parameters.
Seamless-like semantics of checksums-define-the-computation, without forcing the use of Seamless.


The low level is a set of concrete bash script invocations that are to be executed. They closely mirror
the actions.
For now, the user executes them manually. 
Scripts are tied to a folder ("action directory"), 
that contains links to *all* dependencies (including all code files) and parameter
files. A special JSON file contains all checksums (for reuse/deps tracking).
Execution can be: 
    - local mode (direct execution on a workstation or on an interactive shell of a compute node), 
    - Slurm mode (prepend with sbatch depending on generic slurm config) 
    - Seamless mode (preprend with seamless-run depending on seamless.yaml config),
A special --scratch mode indicates that the folder is not to be kept (for long). 
    Local mode: write to /tmp or /scratch and softlink
    Slurm mode: refuse to execute (don't write to network file sys; need to run from compute node)
    Seamless mode: seamless-run --scratch 


Helpers
=======

Helpers are modules (preferably a single file) that can be imported to perform a single function as 
part of an action.

- library.py . Requires configuration to load a fragment library / rotaconformers from files, and to
eliminate/replace conformers based on the PDB code that is being modeled.
Convert the rotations from rotvec to 3x3/quaternion when needed. DONE

- parse_pdb.py. DONE

- rna_pdb.py. Converts a parsed PDB into a list of nucleotides. DONE

- mutate.py. Mutate nucleotide coordinates between A and G, and between C and U. DONE

- offsets.py . Add offsets for each rotaconformer that was generated by the primary grow or connect algo. DONE

- pose reader/writer: See "pose format".

Actions
=======

- A. Anchor
- B. Determine conformer pairs
- C. Grow
- D. Connect
- E. Score poses with ATTRACT
- F. Build (sub)chains
- H. Score subchains with HADDOCK
- I. Apply filter mask
- J. Identity filter
- K. Same-conformer filter (if rel ori unknown)
- L. Propagate
- M. Bond filter
- N. Long-bond filter
- O. Basepairing filter
- P. Intra-RNA stacking filter
- Q. Dedup

Utils
=====

Command-line tools that perform a function unrelated to actions.

- A. Add missing atoms (ATTRACT aareduce using pdb2pqr or mononuc lib)
- B. Calculate RMSD towards a reference structure (with trace math!)
- C. Modify a model (highest level) or command list (mid level) based on a reference structure
- D. Average out chain overlapping nucleotides.
- E. Visualize as PDB file


Actions overview
================

## A. Anchor

Create initial fragment poses from stacking interaction.
Prototype for 2 stackings, using "provisional" classifiier (from nucleotide-stacking-interaction): 
crocodile-OLD/util/pseudo-crocodile-2stack.py

## B. Determine conformer pairs

Determine which conformers the next fragment to grow can have.
Simplest way is by cRMSD. Generates a one-to-many conformer-conformer pairing. Same for common trinucleotides.
Statistical analysis (e.g. Louise's project, or nucleotide-library-fit/generate-words.py) would also be possible. 
In that case, you may get sub-conformer mapping: map each pose to a sub-conformer, which has its own sub-conformer-conformer pairing.
This way, you can restrict the fragN+1 conformers of fragN conformer X differently depending on by which fragN-1 conformer X is preceded.

## C. Grow

Build poses for fragment N+1 (or N-1) based on poses of fragment N. 
Requires ovRMSD threshold, and conformer pairs.
... (trace math!!) ...

## D. Connect

Build poses for fragment N+1 (or N-1) based on poses of fragment N and existing poses of fragment N+1.
Achieves the same as a grow followed by a identity-filter.
However, much more efficient since only the existing rotaconformers of each conformer of fragment N+1
need be considered.

## E. Score poses with ATTRACT

Creates a filter mask based on ATTRACT score (attract-jax, to be modified)
Requires:
- A protein PDB file with no missing heavy atoms
- Dinucleotide template PDB file 
- Pose files, fragment libraries as usual.
Steps:
- Reduce both PDB files to ATTRACT atom types
- On-the-fly compute potential and neighbor list grids
- Convert poses to 4x4 matrices
- Run attract-jax routines


## L. Propagate (rebase)
See "Pose connection and provenance"

## M. Bond filter
For poses in fragN, filter out those whose phosphate is not within bond distance of any pose in fragN+1 (or N-1).
Probably use KDTrees or flat neighbor list grid.
Note that this is an efficiency filter, it won't affect the outcome.

## N. Long-bond filter
As for bond, but between fragN and fragN+X (and much larger distance)

## Q. Dedup
See "Pose connection and provenance"

Concepts

========

## Config
Config for SLURM mode or seamless mode must be present (~/.crocodile)
Fragment libraries are defined as checksum files. On the machine where the action is executed,
  there must be config where to find the files (unless seamless mode, which has its own mechanism).

## General action directory outline
... (checksums, crocodile-init to create RUNNING status)
Pose filtering creates indices (uint32) into original directory, not poses!
Action is nearly always "input dir" => "output dir", by name.
Caching: input directories are inspected for checksum hits, beyond the input dir whose name is specified.
Output dir if exists must have the correct checksum signature, else error.
(in other words, after running, changing an input parameter requires an output dir rename)

### Pose format
"poses.npy": 3 uint16 per pose: conformer, rotamer, offset index.
"offsets.dat". First 6 bytes (3 x int16) is the mean to be added to each offset.
Then, for each offset, 3 int8 values.
If the offset spread in x or y or z is beyond 127 
    OR there are more than 2**16 different offsets 
    OR there are more than 2**32 poses,
 then multiple poses.npy + offsets.dat must be created. (poses-N.npy + offsets-N.dat)

### Pose connection and provenance
The pose list has either an origin list (fragN-1 pose it was grown from)
 or a connection list (many-to-many pose-pose connection edge list)
Dedup is that identical poses are collapsed onto one, this also changes any origin list
into a connection list.

## Init and stats directories
Init dir contains the input data (PDB file, RNA sequence)
Stats directory contains stats about the action directories: number of poses, score distribution, RMSD distribution, etc.

